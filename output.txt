# SimpleAgent2: A Chatbot That Remembers, Learns, and Uses Skills
*(This branch is not working currently.  It's a giant refactoring with a lot of new bells and whistles.  Be patient and help if you can.)*

This chatbot is like a friend you can talk to and ask questions. It's built with some cool AI technology, and it can:

* **Remember your conversations:** So it doesn't forget what you've talked about.
* **Search the internet:** To find answers to your questions.
* **Learn from a huge knowledge base:** To give you even better responses. 
* **Use custom skills:** To perform specific tasks or provide specialized information.

## Getting Started

This chatbot code is hosted on GitHub. Here's how to get a copy on your computer:

1. **Fork the Repository:**
   - Go to the project's GitHub page: https://github.com/MikeyBeez/SimpleAgent2
   - Click the "Fork" button in the top right corner. This will create your own copy of the project that you can work on.

2. **Clone Your Fork:**
   - Go to your forked repository on GitHub. 
   - Click the green "Code" button and copy the URL. 
   - Open your terminal and run the following command, replacing `<your_repository_url>` with the copied URL:
     ```bash
     git clone <your_repository_url>
     ```
   - This will download the code to your computer.

3. **Set Up Your Tools:**
   - You'll need to have [Conda](https://docs.conda.io/en/latest/) installed.
   - Open your terminal (or command prompt) and go into the project directory that you just cloned.
   - Create a special environment for this project:
      ```bash
      conda env create -f environment.yaml -n lg2
      conda activate lg2
      ```
   - Install the necessary Python packages:
     ```bash
     pip install -r requirements.txt
     ```
   - You'll also need [Ollama](https://ollama.com/) installed and running. It's an AI tool that powers the chatbot.
   - Download the language and embedding models for Ollama:
     ```bash
     ollama pull llama3-chatqa
     ollama pull all-minilm
     ```

4. **Prepare the Knowledge Base (RAG):**

   - **The `kb` Directory:** You already have a `kb` directory, so you're good to go!

   - **Download the Dataset:**
     - Download the DBpedia short abstracts dataset from this link: https://databus.dbpedia.org/vehnem/text/short-abstracts/2021.05.01/short-abstracts_lang=en.ttl.bz2
     - Save the downloaded file (it will be named something like `short-abstracts_lang=en.ttl.bz2`) into your `kb` directory.

   - **Unzip the Dataset:**
     - Open your terminal, navigate to the `kb` directory (`cd kb`), and unzip the dataset using this command (replace `short-abstracts_lang=en.ttl.bz2` with the actual name of the file you downloaded):
       ```bash
       bunzip2 -k short-abstracts_lang=en.ttl.bz2
       ```
     - This will create a file named `short-abstracts_lang=en.ttl` in your `kb` directory. 

   - **Prepare the Conversion Tools:**
     - Copy the files `extract_json.py` and `generate_embeddings.py` from the main project directory into the `kb` directory. 

   - **Extract Text from the Dataset:**
     - While still in the `kb` directory in your terminal, run the following script:
       ```bash
       python extract_json.py 
       ```
     - This will create a new file named `extracted_kb.json`.
     - **Important:** This process might take a few hours.

   - **Generate Embeddings:**
     - Run the following script:
       ```bash
       python generate_embeddings.py
       ```
     - This will create a new file named `kb_with_embeddings.json`.
     - **Important:** This can take a very long time (possibly more than 20 hours). 

   - **ChromaDB:** The chatbot uses ChromaDB, a special database, to store and quickly search the knowledge base embeddings. ChromaDB will create its data files in the `my_kb` directory.

5. **Run the Chatbot:**
   - `python main.py`

## Chatting with the Bot

* The chatbot will ask for your name.
* You can type messages and ask it questions. 
* To use a custom skill, start your input with the wakeword "**assistant**" followed by a phrase or keyword that triggers the skill.  
* For example:

* Try asking it things like:
 - "What's the capital of France?"
 - "How do you make a pizza?"
* To clear the chatbot's memory, type: `clear memory`
* To exit the chatbot, type: `quit`, `exit`, or `bye`.

## Making It Your Own

You can change how the chatbot talks and acts:

* **Change its personality:**  Edit the messages in the `prompts.py` file. 
* **Use a different search engine:**  If you don't like DuckDuckGo, you can replace it with another search engine. 
* **Try a different AI model:** Ollama has different models you can experiment with.
* **Add new skills:** Follow the instructions in the **Adding New Skills** section.

## Adding New Skills

1. **Create a Skill Class:**
* Create a new Python file in the project's root directory (for example, `my_new_skill.py`).
* Define a class that inherits from the `Skill` class.
* Implement the `process(self, input_text)` method to perform the skill's action.
* Implement the `trigger(self, input_text)` method to determine when the skill should be activated.  The `trigger` method should check for phrases or keywords *after* the "assistant" wakeword.

2. **Register Your Skill:**
* In the `chat_manager.py` file, import your new skill class.
* Create an instance of your skill class in the `ChatManager`'s `__init__` method.
* Add the instance to the `available_skills` list. 

**To use a custom skill, start your input with the wakeword "assistant" followed by a phrase or keyword that triggers the skill.**

For example, if you have a skill called `TellJokeSkill`, you could trigger it with:  "Assistant. tell a joke."


## If Something Goes Wrong

* **Check the error messages:**  If you get an error message, read it carefully. It might tell you what's wrong.
* **Make sure Ollama is running:**  The chatbot needs Ollama to work.
* **Be patient:**  Extracting information from the knowledge base can take a long time. 

## Want to Help?

* **Report problems:** If you find any bugs, let us know!
* **Share your ideas:**  Have ideas for new features? Tell us about them!

## System Architecture

The chatbot's code is organized into several modules:

**Core Components:**

* **`chat_manager.py`:** Initializes the LLM, search engine, entities, embeddings, and the router. Runs the main conversation loop. 
* **`prompts.py`:**  Defines prompt templates for the LLM.
* **`memory.py`:**  Handles conversation history storage and retrieval.
* **`entities.py`:**  Keeps track of user information.
* **`routing.py`:**  Contains the `Router` class, which decides how to handle user input (search, skills, or knowledge base).

**Refactored Modules:**

* **`search_logic.py`:**  Determines if a web search is needed based on the user's question and conversation history.
* **`context_manager.py`:**  Updates the context vectorstore with new questions for semantic similarity checks.
* **`skill_handler.py`:**  Checks if any registered skills should be triggered and executes them. 

**Knowledge Base:**

- **`kb` directory:**  Contains the knowledge base data and processing scripts.
- **`extract_json.py` and `generate_embeddings.py`:**  Scripts to process and embed the knowledge base. 
- **ChromaDB:** A vector database for efficient knowledge base search. 

**Skills:**

* **`get_time_skill.py`:**  A skill to get the current time.
* **(Add your new skill files here)**

**Main Program:**

* **`main.py`:**  The main program that sets up and runs the chatbot.

**Customization and Extension:**

The modular design makes it easy to customize the chatbot:

* **Prompts:** Modify the prompts in `prompts.py` to change the chatbot's personality.
* **Search Engine:** Replace DuckDuckGo with a different search engine.
* **Embedding Model:** Use different embedding models for semantic similarity.
* **Knowledge Base:** Update or replace the knowledge base data.
* **Skills:** Add new skills to extend the chatbot's capabilities.

Let's chat! ðŸ˜Š 
import json
from langchain_community.embeddings import OllamaEmbeddings
from tqdm import tqdm  # Make sure you have tqdm installed!

# 1. Load the extracted KB data from the JSON file
with open("extracted_kb.json", "r") as f:
    kb_entries = json.load(f)

# 2. Initialize the Ollama embedding model
embeddings = OllamaEmbeddings(model="mxbai-embed-large")  # Replace with your model

# 3. Generate embeddings for each text entry
print("Generating embeddings...")
for entry in tqdm(kb_entries):  # Wrap kb_entries with tqdm for the progress bar
    entry["embedding"] = embeddings.embed_query(entry["text"])

# 4. Save the updated data with embeddings to a new JSON file
with open("kb_with_embeddings.json", "w") as f:
    json.dump(kb_entries, f, indent=4)

print("Embeddings generated and saved to kb/kb_with_embeddings.json") 
from langchain.agents import Tool, AgentExecutor, ZeroShotAgent, AgentOutputParser
from langchain_community.llms import Ollama
from langchain.chains import LLMChain  # No longer needed
from langchain_community.tools import DuckDuckGoSearchRun
import datetime

# Define the Time tool
def get_time():
    now = datetime.datetime.now()
    current_time = now.strftime("%I:%M %p")
    return f"The current time is {current_time}."

time_tool = Tool(
    name="Get Time",
    func=get_time,
    description="Useful for getting the current time."
)

# Define the Search tool
search_tool = Tool(
    name="Search",
    func=DuckDuckGoSearchRun().run,
    description="Useful for when you need to answer questions about current events or look up information."
)

# Custom AgentOutputParser 
class CustomOutputParser(AgentOutputParser):
    def parse(self, llm_output: str) -> str:
        """Parse the output of the LLM."""
        # For this simple example, we'll just return the LLM's output
        # You might need more complex logic for different tools and outputs.
        print(f"LLM output: {llm_output}")
        return llm_output.strip()

# Initialize the LLM 
llm = Ollama(model="llama3-chatqa")

# Create the agent - using RunnableSequence (prompt | llm)
tools = [time_tool, search_tool]
from langchain.prompts import PromptTemplate
prompt = PromptTemplate(
    template="Answer the following question: {question}",
    input_variables=["question"],
)
# agent = ZeroShotAgent(llm_chain=LLMChain(llm=llm, output_parser=CustomOutputParser()), 
#                         tools=tools, 
#                         max_iterations=3)
agent = ZeroShotAgent(llm_chain=prompt | llm, tools=tools, max_iterations=3)
agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)

# Main loop to ask questions
while True:
    user_input = input("You: ")

    if user_input.lower() in ["quit", "exit", "bye"]:
        break

    # Run the agent
    response = agent_executor.run(user_input)
    print(f"Agent: {response}")
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma

# 1. Initialize Embedding Model and Vectorstore
embedding_model = OllamaEmbeddings(model="all-minilm")  # Or your preferred model
vectorstore = Chroma("my_test_context", embedding_function=embedding_model)

# 2. Example Questions and Chat History
questions = [
    "What is the capital of France?",
    "How do I make a cup of tea?",
    "Tell me about artificial intelligence."
]
chat_histories = [
    "User: Hi\nAgent: Hello!",
    "User: What's your name?\nAgent: I'm a chatbot.",
    "User: What can you do?\nAgent: I can answer questions."
]

# 3. Update Context for Each Question
for question, chat_history in zip(questions, chat_histories):
    # Embed the question
    question_embedding = embedding_model.embed_query(question)

    # Add to vectorstore with metadata
    vectorstore.add_texts(
        texts=[question],
        embeddings=[question_embedding],
        metadatas=[{"history": chat_history}]
    )

# 4. Test Retrieval
test_question = "What is the weather like today?"
test_embedding = embedding_model.embed_query(test_question)

# Get similar documents and scores using similarity_search_with_score
results_and_scores = vectorstore.similarity_search_with_score(test_question, k=2) 

# Print results
print("Test Question:", test_question)
for result, score in results_and_scores:
    print("-" * 20)
    print("Similar Question:", result.page_content)
    print("Similarity Score:", score)
    print("Chat History:", result.metadata["history"])

import unittest
from unittest.mock import MagicMock, patch
from update_and_save_context import update_and_save_context
from chat_manager import ChatManager

class TestUpdateAndSaveContext(unittest.TestCase):
    @patch('chat_manager.ChatManager.memory.save_context') 
    @patch('context_manager.update_context')  
    def test_update_and_save_context(self, mock_update_context, mock_save_context):
        chat_manager = ChatManager()
        update_and_save_context("Test input", "Test response", chat_manager)
        mock_update_context.assert_called_once_with("Test input", "Test response")
        mock_save_context.assert_called_once_with(
            {"input": "Test input"}, {"output": "Test response"}
        )

if __name__ == "__main__":
    unittest.main()
import rdflib

# 1. Create an empty RDF graph
graph = rdflib.Graph()

# 2. Parse the N-Triples file into the graph 
graph.parse("kb/short-abstracts_lang=en.nt", format="nt") 

print("N-Triples data loaded into RDF graph!")
zip_code = "72619"
latitude = 36.3760 # Replace with your latitude
longitude = -92.5892 # Replace with your longitude
noaa_weather_token = "ctVbFFiKVXrINLkJSAEsnCAnhLoyoMqO"
embedding_model_name = "all-minilm"  # Or your preferred embedding model
WELCOME_MESSAGE = """
*****************************
**  Welcome to the Chatbot! **
*****************************
"""

import unittest
from get_weather_skill import GetWeatherSkill
from unittest.mock import patch, MagicMock
import config  # Import your config file

class TestGetWeatherSkill(unittest.TestCase):

    @patch('requests.get')
    def test_get_noaa_weather_success(self, mock_get):
        """Tests successful weather retrieval."""
        # Mock location response
        mock_location_response = MagicMock()
        mock_location_response.status_code = 200
        mock_location_response.json.return_value = {
            "properties": {
                "relativeLocation": {
                    "properties": {
                        "city": "Springfield",
                        "state": "MO"
                    }
                },
                "forecast": "https://test-forecast-url.com" 
            }
        }

        # Mock forecast response
        mock_forecast_response = MagicMock()
        mock_forecast_response.status_code = 200
        mock_forecast_response.json.return_value = {
            'properties': {
                'periods': [
                    {
                        'temperature': 75,
                        'windSpeed': '10 mph',
                        'shortForecast': 'Sunny',
                        'isDaytime': True
                    },
                    {
                        'temperature': 60,
                        'windSpeed': '5 mph',
                        'shortForecast': 'Partly Cloudy',
                        'isDaytime': False 
                    }
                ]
            }
        }

        # Set up mock requests.get behavior
        def mock_requests_get(url, headers=None):
            if url == f"https://api.weather.gov/points/{config.latitude},{config.longitude}":
                return mock_location_response
            elif url == "https://test-forecast-url.com": 
                return mock_forecast_response
            else:
                raise ValueError(f"Unexpected URL: {url}")

        mock_get.side_effect = mock_requests_get 

        # Create skill instance and get response
        skill = GetWeatherSkill(latitude=config.latitude, longitude=config.longitude)
        response = skill.process("weather")

        # Assertions
        self.assertIn("Currently, it's 75 degrees in Springfield, MO", response)
        self.assertIn("with a wind speed of 10 mph", response)
        self.assertIn("It's sunny.", response)
        self.assertIn("You can expect more of the same today with a high of 75 and a low of 60.", response)

    @patch('requests.get')
    def test_get_noaa_weather_location_failure(self, mock_get):
        """Tests when the location API call fails."""
        mock_response = MagicMock()
        mock_response.status_code = 404  # Simulate a Not Found error
        mock_get.return_value = mock_response

        skill = GetWeatherSkill(latitude=config.latitude, longitude=config.longitude)
        response = skill.process("weather")

        # Assert None is returned (or adjust based on error handling in your code)
        self.assertIsNone(response)  

    @patch('requests.get')
    def test_get_noaa_weather_forecast_failure(self, mock_get):
        """Tests when the forecast API call fails."""
        mock_location_response = MagicMock()
        mock_location_response.status_code = 200
        mock_location_response.json.return_value = {
            "properties": {
                "relativeLocation": {
                    "properties": {
                        "city": "Springfield",
                        "state": "MO"
                    }
                },
                "forecast": "https://test-forecast-url.com"
            }
        }

        mock_forecast_response = MagicMock()
        mock_forecast_response.status_code = 500  # Simulate a server error
        mock_get.side_effect = [mock_location_response, mock_forecast_response]

        skill = GetWeatherSkill(latitude=config.latitude, longitude=config.longitude)
        response = skill.process("weather")

        # Assert None is returned (or adjust based on error handling in your code)
        self.assertIsNone(response)

if __name__ == "__main__":
    unittest.main()
import json
from langchain_community.embeddings import OllamaEmbeddings
from tqdm import tqdm

# 1. Load the extracted KB data from the JSON file
with open("extracted_kb.json", "r") as f:
    kb_entries = json.load(f)

# 2. Initialize the Ollama embedding model
embeddings = OllamaEmbeddings(model="all-minilm")  

# 3. Generate embeddings in batches
print("Generating embeddings...")
batch_size = 32  # Adjust this based on your available memory

for i in tqdm(range(0, len(kb_entries), batch_size)):
    batch = kb_entries[i: i + batch_size]
    texts = [entry["text"] for entry in batch]

    # Generate embeddings for the batch
    batch_embeddings = embeddings.embed_documents(texts)

    # Assign embeddings to entries in the batch
    for j, embedding in enumerate(batch_embeddings):
        batch[j]["embedding"] = embedding

# 4. Save the updated data with embeddings to a new JSON file
with open("kb_with_embeddings.json", "w") as f:
    json.dump(kb_entries, f, indent=4)

print("Embeddings generated and saved to kb_with_embeddings.json") 
import rdflib

# 1. Create an empty RDF graph
graph = rdflib.Graph()

# 2. Parse the N-Triples file into the graph 
graph.parse("kb/short-abstracts_lang=en.nt", format="nt") 

print("N-Triples data loaded into RDF graph!")
import rdflib
import json
from tqdm import tqdm

# 1. Load the N-Triples file into the graph
print("Starting to load N-Triples file...")
graph = rdflib.Graph()
graph.parse("kb/short-abstracts_lang=en.nt", format="nt")
print("N-Triples file loaded into RDF graph.")

# 2. Extract relevant text (using rdfs:comment)
print("Starting text extraction...")
kb_entries = []
total_triples = 0
extracted_triples = 0

# Wrap the loop with tqdm for a progress bar
for subject, predicate, object in tqdm(graph):  
    total_triples += 1
    if predicate == rdflib.URIRef("http://www.w3.org/2000/01/rdf-schema#comment"):
        kb_entries.append({
            "text": str(object),
            "uri": str(subject)
        })
        extracted_triples += 1

# 3. Save the extracted entries to a JSON file
print("Saving extracted entries to JSON file...") 
with open("extracted_kb.json", "w") as f:
    json.dump(kb_entries, f, indent=4)

print(f"Processed a total of {total_triples} triples.")
print(f"Extracted {extracted_triples} KB entries saved to extracted_kb.json")
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.memory import ConversationBufferMemory
from langchain.schema import BaseChatMessageHistory, HumanMessage, AIMessage
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from pydantic import BaseModel, Extra
from chat_loop_modules.context_manager import get_chat_history

class EmbeddingMemory(ConversationBufferMemory):
    """
    Memory class that uses embeddings to store and retrieve conversation history.
    """

    class Config:
        extra = Extra.allow

    def __init__(self, vectorstore, llm, summary_frequency=5): 
        super().__init__(llm=llm, memory_key="chat_history", input_key="input", output_key="output")
        self.vectorstore = vectorstore  
        self._summary_frequency = summary_frequency
        self.turn_count = 0

    def load_memory_variables(self, question):
        """
        Loads relevant context from the vectorstore based on the current question.
        """
        # Create the inputs dictionary here
        inputs = {"question": question}

        question_embedding = self.vectorstore._embedding_function.embed_query(question)

        # Retrieve top 5 most similar results (not used for now)
        results = self.vectorstore.similarity_search_by_vector(question_embedding, k=5)

        # Get the chat history directly:
        chat_history = get_chat_history() 

        # Load context into ConversationBufferMemory
        for message in chat_history: 
            if message.startswith('User'):
                self.chat_memory.add_user_message(HumanMessage(content=message.split('User: ')[1]))
            elif message.startswith('Chatbot'):
                self.chat_memory.add_ai_message(AIMessage(content=message.split('Chatbot: ')[1]))

        return {"history": chat_history} 

    def save_context(self, inputs, outputs):
        """
        Saves the current interaction to memory and the vectorstore. 
        """
        text = f"User: {inputs['input']}\nChatbot: {outputs['output']}"
        embedding = self.vectorstore._embedding_function.embed_query(text) 

        # Provide a dictionary or None for metadatas
        metadata = {"source": "conversation"} 
        self.vectorstore.add_texts([text], embeddings=[embedding], metadatas=[metadata]) 

        # Update the base ConversationBufferMemory 
        super().save_context(inputs, outputs) 

        self.turn_count += 1
        if self.turn_count % self._summary_frequency == 0:  
            self.summarize_history(inputs, outputs)

    def summarize_history(self, inputs, outputs):
        """
        Summarizes the conversation history using the LLM.
        """
        messages = self.chat_memory.messages
        if len(messages) == 0:
            return

        summarization_prompt = ChatPromptTemplate.from_messages(
            [
                MessagesPlaceholder(variable_name="chat_history"),
                (
                    "system",
                    "Condense the above chat messages into a single, concise summary message. "
                    "Include important details and user requests."
                ),
            ]
        )
        summarization_chain = summarization_prompt | self.llm

        summary_message = summarization_chain.invoke({"chat_history": messages})

        self.chat_memory.clear()  
        self.chat_memory.add_message(summary_message) 
from prompts import SHOULD_SEARCH_PROMPT
import spacy
from scipy.spatial.distance import cosine

def should_search(question, chat_history, llm, embeddings, chat_history_embeddings):
    # Check for common greetings
    common_greetings = ["hello", "hi", "hey", "good morning", 
                        "good afternoon", "good evening", "good night", 
                        "goodbye", "bye", "see you later", "talk to you later"]
    if question.lower() in common_greetings:
        return False

    # Check for simple questions
    simple_questions = ["how are you?", "what's up?", "how's it going?", 
                        "what's your name?", "what can you do?", "what's the time?"]
    if question.lower() in simple_questions:
        return False

    # Check for contextual similarity
    question_embedding = embeddings.embed_query(question)
    if chat_history_embeddings:
        similarities = [1 - cosine(question_embedding, embedding) 
                        for embedding in chat_history_embeddings]
        most_similar_index = similarities.index(max(similarities))
        most_similar_score = similarities[most_similar_index]
        if most_similar_score > 0.8:  # Adjust threshold as needed
            return False

    # If none of the checks pass, use the LLM to determine if a search is needed
    response = llm.invoke(
        SHOULD_SEARCH_PROMPT.format(question=question, chat_history=chat_history)
    ).strip()
    return response.lower() == "yes"
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
import config
import json 

embedding_model = OllamaEmbeddings(model=config.embedding_model_name)
vectorstore = Chroma("my_context", embedding_function=embedding_model)

chat_history = [] # Store the chat history directly in context_manager.py

def update_context(question, response):  
    """
    Updates the context vectorstore and chat history.
    """
    print(f"Updating context with question: {question}")

    # Append question and response to history 
    chat_history.append(f"User: {question}") 
    chat_history.append(f"Agent: {response}")

    # Embed question and response
    question_embedding = embedding_model.embed_query(question)
    response_embedding = embedding_model.embed_query(response)

    # Convert chat_history to a JSON string
    chat_history_json = json.dumps(chat_history)

    # Add to vectorstore
    vectorstore.add_texts(
        texts=[question, response],
        embeddings=[question_embedding, response_embedding],
        metadatas=[{"history": chat_history_json}]
    )

def get_chat_history():
    """Returns the current chat history."""
    return chat_history 
import logging
from prompts import MAIN_PROMPT

def generate_response(user_input, response, chat_manager):
    """Formats the prompt and generates a response from the LLM."""
    logging.info("Formatting prompt...")
    user_name = chat_manager.entities.get_user_name()
    formatted_prompt = MAIN_PROMPT.format(
        chat_history=chat_manager.memory.buffer,
        question=user_input,
        search_results=response,
        user_name=user_name
    )
    logging.info(f"Formatted prompt: {formatted_prompt}")

    logging.info("Generating response...")
    llm_response = ""
    for chunk in chat_manager.llm.stream(formatted_prompt, temperature=0.7):
        llm_response += chunk
        print(chunk, end="", flush=True)
        logging.info(f"Response chunk: {chunk}")
    logging.info("Response generation complete.")
    print()

    logging.info(f"Response: {llm_response}")
    return llm_response 
import unittest
from unittest.mock import patch
from handle_special_commands import handle_special_commands
#from chat_manager import ChatManager
import chat_manager

class TestHandleSpecialCommands(unittest.TestCase):
    @patch('handle_special_commands.exit')  
    @patch('chat_manager.ChatManager.memory.vectorstore.delete_collection')
    def test_handle_clear_memory(self, mock_delete_collection, mock_exit):
        chat_manager = ChatManager()
        result = handle_special_commands("clear memory", chat_manager)
        self.assertTrue(result)  
        mock_delete_collection.assert_called_once() 

    @patch('handle_special_commands.exit')
    def test_handle_exit_commands(self, mock_exit):
        chat_manager = ChatManager()
        for command in ["quit", "exit", "bye"]:
            result = handle_special_commands(command, chat_manager)
            self.assertTrue(result)
            mock_exit.assert_called_once()  
            mock_exit.reset_mock()  

    def test_handle_other_commands(self):
        chat_manager = ChatManager()
        result = handle_special_commands("What is the weather?", chat_manager)
        self.assertFalse(result)  

if __name__ == "__main__":
    unittest.main()
import logging
import config
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from embedding_memory import EmbeddingMemory
from chat_manager import ChatManager

def initialize_user(chat_manager):
    """Gets the user's name and sets it in the ChatManager."""
    logging.info("Initializing user...")
    user_name = input("What is your name? ")
    print(f"Hello, {user_name}!")
    print("How can I help you?")
    chat_manager.entities.set_user_name(user_name)
    logging.info(f"User name set to {user_name}")

def initialize_chatbot():
    """
    Performs chatbot initialization tasks, including ChromaDB setup and user setup.
    """
    logging.basicConfig(
        filename='chat_log.txt',
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(filename)s - %(message)s'
    )
    logging.info("Chatbot initialization started.")

    print(config.WELCOME_MESSAGE)

    # 1. Initialize Embedding Model 
    embedding_model = OllamaEmbeddings(model=config.embedding_model_name)

    # 2. Check if the ChromaDB collection exists
    persist_directory = "my_kb"  # Directory to store ChromaDB data
    collection_name = "my_chat_history"

    # Correctly check for the collection's existence:
    try:
        # Try to get the collection
        Chroma(persist_directory=persist_directory, embedding_function=embedding_model)._collection
        logging.info("ChromaDB collection already exists. Loading...")
    except:
        logging.info("Creating a new ChromaDB collection.")
        # Create a new collection if it doesn't exist
        Chroma.from_texts(
            texts=["Welcome to the chatbot!"],  # Initial text
            embedding=embedding_model,
            collection_name=collection_name,
            persist_directory=persist_directory
        )

    # 3. Initialize ChromaDB 
    vectorstore = Chroma(
        collection_name=collection_name, 
        embedding_function=embedding_model,
        persist_directory=persist_directory
    )

    # 4. Initialize ChatManager
    chat_manager = ChatManager()

    # 5. Initialize EmbeddingMemory using the loaded or created vectorstore
    chat_manager.memory = EmbeddingMemory(vectorstore, chat_manager.llm, summary_frequency=3) 

    # 6. Set Other ChatManager Variables
    chat_manager.zip_code = config.zip_code
    chat_manager.latitude = config.latitude
    chat_manager.longitude = config.longitude
    chat_manager.noaa_weather_token = config.noaa_weather_token
    chat_manager.embedding_model_name = config.embedding_model_name

    # 7. Initialize the User
    initialize_user(chat_manager)

    print("Chatbot initialized. Ready to chat!")
    return chat_manager
import logging

def handle_special_commands(user_input, chat_manager):
    """Handles special commands like 'clear memory' and exit commands."""
    logging.info("Handling special commands...")
    if user_input.lower() == "clear memory":
        logging.info("Clearing memory...")
        chat_manager.memory.vectorstore.delete_collection()
        print("Conversation memory cleared.")
        logging.info("Memory cleared.")
        return True

    if user_input.lower() in ["quit", "exit", "bye"]:
        logging.info("Exiting conversation loop...")
        exit()  # Exit the program
        return True

    return False
import asyncio
import logging
from prompts import MAIN_PROMPT
from .context_manager import update_context, get_chat_history

# Configure logging
logging.basicConfig(
    filename='chat_log.txt',
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(filename)s - %(funcName)s - %(message)s' 
)

async def run_conversation(chat_manager):
    """Handles the main conversation loop."""
    logging.info("Starting conversation loop...")

    while True:
        # Get user input
        user_name = chat_manager.entities.get_user_name()
        if user_name:
            question = input(f"{user_name}: ")
        else:
            question = input("You: ")

        logging.info(f"User Input: {question}")

        # Handle special commands like "clear memory" and exit
        if question.lower() == "clear memory":
            logging.info("Clearing memory...")
            chat_manager.memory.vectorstore.delete_collection()
            print("Conversation memory cleared.")
            logging.info("Memory cleared.")
            continue

        if question.lower() in ["quit", "exit", "bye"]:
            logging.info("Exiting conversation loop...")
            break

        # Check for skill invocation
        if "assistant" in question.lower(): 
            logging.info("Assistant keyword detected, calling skill handler.")
            response = chat_manager.router.route(question, chat_manager.available_skills)
            print(f"Agent: {response}")
            update_context(question, response)
            continue  # Go to the next user input

        # Load relevant context from memory using embeddings
        logging.info("Loading context from memory...")
        context = chat_manager.memory.load_memory_variables({"question": question})
        chat_history = context.get("history", "")
        logging.info(f"Loaded chat history: {chat_history}")

        # Route the question (for regular questions, not skills)
        logging.info("Routing question...")
        response = chat_manager.router.route(question, chat_history, chat_manager.available_skills)
        logging.info("Routing complete.")

        # Check if the response is None (no route found)
        if response is None:
            logging.info("No route found, defaulting response...")
            response = "I'm not sure how to answer that."
            logging.info(f"Default response: {response}")

        # Format the main prompt 
        logging.info("Formatting prompt...")
        formatted_prompt = MAIN_PROMPT.format(
            chat_history=chat_history,
            question=question,
            search_results=response,
            user_name=user_name
        )
        logging.info(f"Formatted prompt: {formatted_prompt}")

        # Generate the agent's response using streaming output
        print("Agent: ", end="")
        response = ""
        logging.info("Generating response...")
        for chunk in chat_manager.llm.stream(formatted_prompt, temperature=0.7):
            response += chunk
            print(chunk, end="", flush=True)
            logging.info(f"Response chunk: {chunk}")
        logging.info("Response generation complete.")
        print()

        # Log the response
        logging.info(f"Response: {response}")

        # Update the context vectorstore 
        logging.info("Updating context...")
        update_context(question, response) 
        logging.info("Context updated.")

        # Save the current interaction to memory (using embeddings)
        logging.info("Saving context to memory...")
        chat_manager.memory.save_context({"input": question}, {"output": response})
        logging.info("Context saved.") 
import logging
from get_time_skill import GetTimeSkill
from get_weather_skill import GetWeatherSkill

# Configure logging
logging.basicConfig(
    filename='chat_log.txt',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def handle_skills(question, available_skills): # Removed chat_history
    logging.info(f"Skill Handler - Question received in handle_skills(): {question}")

    parts = question.lower().split("assistant")  

    if len(parts) > 1:
        skill_trigger = parts[1].strip()
        logging.info(f"Skill Handler - Skill trigger: {skill_trigger}") 

        for skill in available_skills:
            logging.info(f"Skill Handler - Checking skill: {skill.__class__.__name__}")
            if skill.trigger(skill_trigger):
                logging.info(f"Skill Handler - Skill triggered: {skill.__class__.__name__}")
                response = skill.process(question) # Only pass question
                logging.info(f"Skill Handler - Response from skill: {response}")
                return response

    logging.info("Skill Handler - No skill matched the command.")
    return False
import logging

def get_user_input(chat_manager):
    """Gets user input from the command line."""
    logging.info("Getting user input...")
    user_name = chat_manager.entities.get_user_name()
    if user_name:
        user_input = input(f"{user_name}: ")
    else:
        user_input = input("You: ")
    logging.info(f"User Input: {user_input}")
    return user_input
import unittest
from context_manager import update_context, get_chat_history, chat_history

class ContextManagerTests(unittest.TestCase):

    def setUp(self):
        """Clears the chat history before each test."""
        global chat_history  # Access the global chat_history variable
        chat_history.clear()

    def test_update_context(self):
        """Tests that update_context adds messages to history and updates the vectorstore."""
        update_context("What is the capital of France?", "Paris is the capital of France.")
        update_context("How are you?", "I am an AI, so I don't have feelings, but I'm here to help!")

        # Check if the chat history is updated correctly
        expected_history = [
            "User: What is the capital of France?",
            "Agent: Paris is the capital of France.",
            "User: How are you?",
            "Agent: I am an AI, so I don't have feelings, but I'm here to help!"
        ]
        self.assertEqual(get_chat_history(), expected_history)

        # (Optional) Add assertions to check the vectorstore content 
        # You'll need to access the vectorstore and verify 
        # that the embeddings and metadata are added correctly.

    def test_get_chat_history(self):
        """Tests that get_chat_history returns the correct history."""
        # Add some test data to the chat history
        update_context("Test Question 1", "Test Answer 1")
        update_context("Test Question 2", "Test Answer 2")

        expected_history = [
            "User: Test Question 1",
            "Agent: Test Answer 1",
            "User: Test Question 2",
            "Agent: Test Answer 2"
        ]
        self.assertEqual(get_chat_history(), expected_history)

if __name__ == '__main__':
    unittest.main()
import unittest
from unittest.mock import patch
from io import StringIO
from get_user_input import get_user_input
from chat_manager import ChatManager

class TestGetUserInput(unittest.TestCase):
    @patch('builtins.input', return_value="Test input")
    def test_get_user_input_with_name(self, mock_input):
        chat_manager = ChatManager()
        chat_manager.entities.set_user_name("Test User")
        user_input = get_user_input(chat_manager)
        self.assertEqual(user_input, "Test input")

    @patch('builtins.input', return_value="Test input")
    def test_get_user_input_without_name(self, mock_input):
        chat_manager = ChatManager()
        user_input = get_user_input(chat_manager)
        self.assertEqual(user_input, "Test input")

if __name__ == "__main__":
    unittest.main()
import unittest
from unittest.mock import MagicMock, patch
from io import StringIO
from generate_response import generate_response
from chat_manager import ChatManager

class TestGenerateResponse(unittest.TestCase):
    @patch('sys.stdout', new_callable=StringIO)
    @patch('chat_manager.ChatManager.llm.agenerate_stream')  # Replace 'agenerate_stream' with the actual method name
    def test_generate_response(self, mock_stream, mock_stdout):
        chat_manager = ChatManager()
        mock_stream.return_value = ["This is ", "a test ", "response."]
        response = generate_response("Test question", "Test search results", chat_manager)
        self.assertEqual(response, "This is a test response.")

        # Check printed output
        output = mock_stdout.getvalue().strip()
        self.assertEqual(output, "This is a test response.")

if __name__ == "__main__":
    unittest.main()
import unittest
from unittest.mock import patch
from io import StringIO  
from initialize import initialize_user
from chat_manager import ChatManager

class InitializeTests(unittest.TestCase):

    @patch('builtins.input', return_value="Test User")
    @patch('sys.stdout', new_callable=StringIO)  
    def test_initialize_user(self, mock_stdout, mock_input):
        chat_manager = ChatManager()
        initialize_user(chat_manager)

        # Check if user_name is set correctly in ChatManager
        self.assertEqual(chat_manager.entities.get_user_name(), "Test User")

        # Check the printed output (optional)
        output = mock_stdout.getvalue().strip()
        self.assertIn("Hello, Test User!", output)
        self.assertIn("How can I help you?", output)

if __name__ == '__main__':
    unittest.main()
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.memory import ConversationBufferMemory
from langchain.schema import BaseChatMessageHistory, HumanMessage, AIMessage
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from pydantic import BaseModel, Extra
from chat_loop_modules.context_manager import get_chat_history
import logging

logging.basicConfig(
    filename='chat_log.txt',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(filename)s - %(funcName)s - %(message)s'
)

class EmbeddingMemory(ConversationBufferMemory):
    """
    Memory class using embeddings to store and retrieve conversation history.
    """

    class Config:
        extra = Extra.allow

    def __init__(self, vectorstore, llm, summary_frequency=5):
        super().__init__(llm=llm, memory_key="chat_history", input_key="input", output_key="output")
        self.vectorstore = vectorstore
        self._summary_frequency = summary_frequency
        self.turn_count = 0

    def load_memory_variables(self, question):
        """Loads relevant context from vectorstore based on the current question."""
        logging.info("Loading memory variables")

        inputs = {"question": question}
        question_embedding = self.vectorstore._embedding_function.embed_query(question)

        # Retrieve top 5 most similar results (not used for now)
        results = self.vectorstore.similarity_search_by_vector(question_embedding, k=5)

        # Get the chat history directly:
        chat_history = get_chat_history()

        # Load context into ConversationBufferMemory
        for message in chat_history:
            if message.startswith('User'):
                self.chat_memory.add_user_message(HumanMessage(content=message.split('User: ')[1]))
            elif message.startswith('Chatbot'):
                self.chat_memory.add_ai_message(AIMessage(content=message.split('Chatbot: ')[1]))

        return {"history": chat_history}

    def save_context(self, inputs, outputs):
        """Saves the current interaction to memory and the vectorstore. """
        logging.info("Saving context")
        text = f"User: {inputs['input']}\nChatbot: {outputs['output']}"
        embedding = self.vectorstore._embedding_function.embed_query(text)

        # Provide a dictionary or None for metadatas
        metadata = {"source": "conversation"}
        self.vectorstore.add_texts([text], embeddings=[embedding], metadatas=[metadata])

        # Update the base ConversationBufferMemory
        super().save_context(inputs, outputs)

        self.turn_count += 1
        if self.turn_count % self._summary_frequency == 0:
            self.summarize_history(inputs, outputs)

    def summarize_history(self, inputs, outputs):
        """Summarizes the conversation history using the LLM."""
        logging.info("Summarizing history")
        messages = self.chat_memory.messages
        if len(messages) == 0:
            return

        summarization_prompt = ChatPromptTemplate.from_messages(
            [
                MessagesPlaceholder(variable_name="chat_history"),
                (
                    "system",
                    "Condense the above chat messages into a single, concise summary message. "
                    "Include important details and user requests."
                ),
            ]
        )
        summarization_chain = summarization_prompt | self.llm

        summary_message = summarization_chain.invoke({"chat_history": messages})

        self.chat_memory.clear()
        self.chat_memory.add_message(summary_message)
import unittest
from unittest.mock import MagicMock
from route_question import route_question
from chat_manager import ChatManager

class TestRouteQuestion(unittest.TestCase):
    def test_route_question(self):
        chat_manager = ChatManager()
        chat_manager.router.route = MagicMock(return_value="Test response")  # Mock the route method
        response = route_question("Test question", chat_manager)
        self.assertEqual(response, "Test response")
        chat_manager.router.route.assert_called_once_with(
            "Test question", [], chat_manager.available_skills  # Pass the expected arguments
        )

if __name__ == "__main__":
    unittest.main()
import unittest
from get_time_skill import GetTimeSkill

class TestGetTimeSkill(unittest.TestCase):
    def test_process(self):
        skill = GetTimeSkill()
        response = skill.process("any input")  # Input doesn't matter for this skill
        self.assertIn("The current time is", response)

if __name__ == "__main__":
    unittest.main()
import logging
from context_manager import update_context

def update_and_save_context(user_input, response, chat_manager):
    """Updates the context and saves it to ChromaDB."""
    logging.info("Updating context...")
    update_context(user_input, response)
    logging.info("Context updated.")

    logging.info("Saving context to memory...")
    chat_manager.memory.save_context({"input": user_input}, {"output": response})
    logging.info("Context saved.") 
from rdflib import Graph

# Load the TTL file into an RDF graph
graph = Graph()
graph.parse("short-abstracts_lang=en.ttl", format="turtle")

# Serialize the graph to N-Triples format
graph.serialize(destination="short-abstracts_lang=en.nt", format="nt", encoding='utf-8')

print("Conversion complete! File saved as short-abstracts_lang=en.nt") 
from langchain.prompts import PromptTemplate

SYSTEM_MESSAGE = """<|begin_of_text|>
<|start_header_id|>system<|end_header_id|>
You are a friendly and enthusiastic AI assistant. 
You have access to a search engine to gather information. 

Refer to the previous conversation to maintain context and avoid repeating information.

Before answering, carefully read and analyze the search results provided.
Use the search results to provide evidence for your answers and cite relevant sources.

Think step-by-step to ensure your reasoning is sound.

Don't repeat yourself over and over.

Don't respond with more than five paragraphs.  

Provide your answer in the form of a concise report, summarizing the relevant findings from the search and incorporating the conversation history.

When summarizing the conversation, focus on the key topics and questions discussed. Only include the most important information. 

Here's an example interaction:

User: What is the capital of France?
Search Results: ... (search results about Paris)
Assistant: The capital of France is Paris. This information was confirmed by searching online. 

<|eot_id|>
<|start_header_id|>user<|end_header_id|>
"""

# Simplified MAIN_PROMPT (no Jinja needed)
MAIN_PROMPT = PromptTemplate(
    input_variables=["chat_history", "question", "search_results", "user_name"],
    template=r"""
    {chat_history}
    {user_name}: {question}

    {search_results}

    Answer:""" 
)

SHOULD_SEARCH_PROMPT = PromptTemplate(
    input_variables=["question", "chat_history"],
    template="""<|begin_of_text|>
    <|start_header_id|>system<|end_header_id|>
    Given the user's question and the previous conversation, determine if a search is necessary to provide a helpful and accurate answer. 

    Respond with "yes" if a search is needed, and "no" if the question can be answered without a search.

    Question: {{question}}
    Chat History:
    {{chat_history}}
    <|eot_id|>
    <|start_header_id|>assistant<|end_header_id|>
    Answer:"""
)
# entities_test.py
import unittest
from entities import Entities

class TestEntities(unittest.TestCase):
    def test_set_and_get_user_name(self):
        entities = Entities()
        entities.set_user_name("Alice")
        self.assertEqual(entities.get_user_name(), "Alice")

    def test_get_user_name_not_set(self):
        entities = Entities()
        self.assertIsNone(entities.get_user_name())

if __name__ == "__main__":
    unittest.main()
import logging

def route_question(user_input, chat_manager):
    """Routes the question to the appropriate handler (skills, search, or knowledge base)."""
    logging.info("Routing question...")
    # Load relevant context from memory using embeddings
    logging.info("Loading context from memory...")
    context = chat_manager.memory.load_memory_variables({"question": user_input})
    chat_history = context.get("history", "")
    logging.info(f"Loaded chat history: {chat_history}")
    response = chat_manager.router.route(
        user_input, chat_history, chat_manager.available_skills
    )
    logging.info("Routing complete.")
    return response
import datetime

class GetTimeSkill:
    def __init__(self):
        pass  # Remove print_current_time() from __init__

    def process(self, input_text):
        now = datetime.datetime.now()
        current_time = now.strftime("%I:%M %p")
        return f"The current time is {current_time}."

    # Remove the trigger() method

    def print_current_time(self): # You can keep this for debugging
        now = datetime.datetime.now()
        current_time = now.strftime("%I:%M %p")
        print(f"The current time is: {current_time}")

# Make similar changes to GetWeatherSkill
from chat_loop_modules.search_logic import should_search
from chat_loop_modules.context_manager import update_context
from chat_loop_modules.skill_handler import handle_skills
from langchain_community.tools import DuckDuckGoSearchRun
import logging

# Configure logging
logging.basicConfig(
    filename='chat_log.txt',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(filename)s - %(funcName)s - %(message)s'
)

DO_NOTHING_TOKEN = "##DO_NOTHING##"

class Router:
    """
    Determines the best approach to answering a user's question.
    """

    def __init__(self, llm, embeddings):
        self.llm = llm
        self.embeddings = embeddings
        self.chat_history_embeddings = []
        self.search = DuckDuckGoSearchRun()

    def route(self, question, available_skills, chat_history=None):
        """
        Determines whether to search, use the knowledge base, or execute a skill.

        Args:
            question (str): The user's question.
            chat_history (str): The conversation history.
            available_skills (list): A list of available Skill objects.

        Returns:
            str: The response to the user, or DO_NOTHING_TOKEN if a skill was handled.
        """
        logging.info(f"User question: {question}")  

        # Check for "assistant" wakeword
        if question.lower().startswith("assistant"):
            logging.info("Assistant wakeword detected.")
            command = question.lower().split("assistant", 1)[1].strip().lstrip(" ,.;:")
            logging.info(f"Extracted command: {command}") 

            # Call handle_skills() only inside the "assistant" wakeword block
            skill_response = handle_skills(command, available_skills)
            if skill_response:
                logging.info("Skill triggered successfully.")
                return DO_NOTHING_TOKEN  
            else:
                logging.info("No skill matched the command.")

        # Regular routing logic (for questions without the "assistant" wakeword) 
        if should_search(question, chat_history, self.llm, self.embeddings, self.chat_history_embeddings):
            logging.info("Decision: Performing a search.") 
            search_results = self.search.run(question)
            return search_results
        else:
            logging.info("Decision: Using knowledge base (not implemented).") 
            return "I don't know."

        update_context(question, response)  
import requests
import config  

class GetWeatherSkill: 
    def __init__(self, latitude, longitude):
        self.latitude = latitude
        self.longitude = longitude

    def process(self, input_text): 
        """Fetches and formats weather data from NOAA."""
        return self.get_noaa_weather(self.latitude, self.longitude)  # Call as method

    def trigger(self, input_text):
        return input_text.lower() == "weather"

    def get_noaa_weather(self, latitude, longitude):
        """Fetches basic weather data from NOAA, 
           includes error handling and checks for dangerous weather, 
           and formats the output for an Alexa-like response.
        """

        base_url = "https://api.weather.gov/points/"
        complete_url = f"{base_url}{latitude},{longitude}"

        headers = {
            "Authorization": f"Bearer {config.noaa_weather_token}"
        }

        try:
            response = requests.get(complete_url, headers=headers)

            if response.status_code == 200:
                data = response.json()

                try:
                    # Get location information
                    city = data['properties']['relativeLocation']['properties']['city']
                    state = data['properties']['relativeLocation']['properties']['state']

                    # Get forecast URL
                    forecast_url = data['properties']['forecast']
                    forecast_response = requests.get(forecast_url, headers=headers)

                    if forecast_response.status_code == 200:
                        forecast_data = forecast_response.json()

                        # Extract current weather details
                        current_period = forecast_data['properties']['periods'][0]
                        current_temp = current_period['temperature']
                        current_wind_speed = current_period['windSpeed']
                        current_short_forecast = current_period['shortForecast'].lower()

                        # Extract today's high and low temperatures
                        high_temp = None
                        low_temp = None
                        for period in forecast_data['properties']['periods']:
                            if period['isDaytime']:
                                high_temp = period['temperature']
                            else:
                                low_temp = period['temperature']
                            if high_temp is not None and low_temp is not None:
                                break

                        # Construct the Alexa-like response
                        weather_summary = (
                            f"Currently, it's {current_temp} degrees in {city}, {state} "
                            f"with a wind speed of {current_wind_speed}. "
                            f"It's {current_short_forecast}. "
                        )
                        if high_temp is not None and low_temp is not None:
                            weather_summary += (
                                f"You can expect more of the same today with a high of "
                                f"{high_temp} and a low of {low_temp}."
                            )

                        return weather_summary  # Return the formatted summary

                    else:
                        print(f"Error fetching forecast data: Status Code {forecast_response.status_code}")
                except KeyError as e:
                    print(f"Error: Unable to find weather data in response. {e}")
            elif response.status_code == 404:
                print("Error 404: Resource not found. Verify coordinates and API endpoint.")
            else:
                print(f"Error: Status code {response.status_code}. Check API documentation.")

        except requests.exceptions.RequestException as e:
            print(f"Error making API request: {e}")
import unittest
from unittest.mock import MagicMock, ANY
from langchain_community.llms import Ollama
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from memory import EmbeddingMemory

class MemoryTests(unittest.TestCase):

    def setUp(self):
        """Sets up the test environment."""
        # Create a mock vectorstore and LLM
        self.vectorstore = MagicMock(spec=Chroma)
        self.llm = MagicMock(spec=Ollama)

        # Set the _embedding_function attribute on the mock vectorstore
        self.vectorstore._embedding_function = MagicMock(spec=OllamaEmbeddings)

        # Initialize EmbeddingMemory with mocks
        self.memory = EmbeddingMemory(self.vectorstore, self.llm)

    def test_load_memory_variables(self):
        """Tests loading memory variables with chat history."""

        def mock_get_chat_history():
            return ["User: Hello", "Chatbot: Hi there!"]

        # Call load_memory_variables with the mock function
        context = self.memory.load_memory_variables(
            {"question": "What's up?"}, 
            mock_get_chat_history  # Pass the function as an argument
        )

        # Assertions
        self.assertEqual(context.get("history"), ["User: Hello", "Chatbot: Hi there!"])

        # (Optional): Assert that the vectorstore's similarity search was called 
        # self.vectorstore.similarity_search_by_vector.assert_called()

    def test_save_context(self):
        """Tests saving context to memory."""
        self.memory.save_context({"input": "What's the weather?"}, {"output": "Sunny today!"})

        # Assert that the vectorstore's add_texts method was called
        self.vectorstore.add_texts.assert_called_once_with(
            ['User: What\'s the weather?\nChatbot: Sunny today!'], 
            embeddings=ANY, 
            metadatas=[{'source': 'conversation'}]
        ) 

if __name__ == '__main__':
    unittest.main()
class Entities:
    """
    Handles user names and other entities.
    """

    def __init__(self):
        self.user_name = None

    def set_user_name(self, name):
        """
        Sets the user's name.

        Args:
            name (str): The user's name.
        """
        self.user_name = name

    def get_user_name(self):
        """
        Returns the user's name.

        Returns:
            str: The user's name, or None if it hasn't been set.
        """
        return self.user_name
import sys
import os

sys.path.append(os.path.abspath(os.path.dirname(__file__)))

import asyncio
import warnings
from chat_loop_modules.chat_loop import run_conversation
from chat_loop_modules.initialize import initialize_chatbot

async def main():
    """Initializes and runs the chatbot."""
    chat_manager = initialize_chatbot()
    await run_conversation(chat_manager)

if __name__ == "__main__":
    warnings.filterwarnings("ignore", category=FutureWarning)
    asyncio.run(main())
import rdflib
import json

# 1. Load the N-Triples file into the graph
graph = rdflib.Graph()
graph.parse("kb/short-abstracts_lang=en.nt", format="nt")

# 2. Extract relevant text (e.g., labels)
kb_entries = []
for subject, predicate, object in graph:
    if predicate == rdflib.URIRef("http://www.w3.org/2000/01/rdf-schema#label"): 
        kb_entries.append({
            "text": str(object), 
            "uri": str(subject)
        })

# 3. Save the extracted entries to a JSON file
with open("kb/extracted_kb.json", "w") as f:
    json.dump(kb_entries, f, indent=4)  # Save with pretty-printing

print("Extracted KB entries saved to kb/extracted_kb.json") 
import unittest
from unittest.mock import patch
from io import StringIO
from initialize import initialize_chatbot, initialize_user
from chat_manager import ChatManager
import config  # Import your config.py 

class InitializeTests(unittest.TestCase):

    # ... (Your existing test_initialize_user function) ...

    @patch('builtins.input', return_value="Test User") # For initialize_user call
    @patch('sys.stdout', new_callable=StringIO)
    def test_initialize_chatbot(self, mock_stdout, mock_input):
        chat_manager = ChatManager()
        initialize_chatbot(chat_manager)

        # Assertions for variables loaded from config.py
        self.assertEqual(chat_manager.zip_code, config.zip_code)
        self.assertEqual(chat_manager.latitude, config.latitude)
        self.assertEqual(chat_manager.longitude, config.longitude)
        self.assertEqual(chat_manager.noaa_weather_token, config.noaa_weather_token)
        self.assertEqual(chat_manager.embedding_model_name, config.embedding_model_name)

        # Check printed output (welcome message and "Ready to chat!")
        output = mock_stdout.getvalue().strip()
        self.assertIn(config.WELCOME_MESSAGE.strip(), output)
        self.assertIn("Chatbot initialized. Ready to chat!", output)

if __name__ == '__main__':
    unittest.main()
import json
import requests
import config
import asyncio

from langchain_community.llms import Ollama
from langchain_community.tools import DuckDuckGoSearchRun
from prompts import MAIN_PROMPT, SHOULD_SEARCH_PROMPT
from entities import Entities
from routing import Router
from langchain_community.embeddings import OllamaEmbeddings
from chat_loop_modules.context_manager import update_context
from get_time_skill import GetTimeSkill
from get_weather_skill import GetWeatherSkill
from embedding_memory import EmbeddingMemory
from langchain_community.vectorstores import Chroma
import logging

# Set up logging
logging.basicConfig(filename='chat_log.txt', level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

class ChatManager:
    """
    Manages the conversation flow, including memory, routing, and responses.
    """
    def __init__(self):
        # Initialize the Ollama language model
        logging.info("Initializing Ollama language model...")
        self.llm = Ollama(model="llama3-chatqa")
        logging.info("Ollama language model initialized.")

        # Initialize the DuckDuckGo search tool
        logging.info("Initializing DuckDuckGo search tool...")
        self.search = DuckDuckGoSearchRun()
        logging.info("DuckDuckGo search tool initialized.")

        # Create an instance of the Entities class (for user info)
        logging.info("Initializing Entities class...")
        self.entities = Entities()
        logging.info("Entities class initialized.")

        # Initialize the embedding model using the config file
        logging.info("Initializing embedding model...")
        self.embedding_model = OllamaEmbeddings(model=config.embedding_model_name)
        self.vectorstore = Chroma("my_chat_history", embedding_function=self.embedding_model)
        logging.info("Embedding model initialized.")

        # Initialize EmbeddingMemory 
        logging.info("Initializing EmbeddingMemory...")
        self.memory = EmbeddingMemory(self.vectorstore, self.llm, summary_frequency=3) 
        logging.info("EmbeddingMemory initialized.")

        # Initialize available skills
        logging.info("Initializing skills...")
        self.get_time_skill = GetTimeSkill()
        self.get_weather_skill = GetWeatherSkill(latitude=config.latitude, longitude=config.longitude)
        self.available_skills = [self.get_time_skill, self.get_weather_skill]
        logging.info("Skills initialized.")

        # Initialize the router with OllamaEmbeddings 
        logging.info("Initializing router...")
        self.embeddings = OllamaEmbeddings(model="all-minilm")
        self.router = Router(self.llm, self.embeddings)
        logging.info("Router initialized.")

        # NO RETURN STATEMENT HERE

    async def run_conversation(self):
        """
        Handles the main conversation loop. 
        """
        logging.info("Starting conversation loop...")

        # Get the user's name if it's not already set
        if not self.entities.get_user_name():
            user_name = input("What is your name? ")
            print(f"Hello, {user_name}!")
            print("How can I help you?")
            self.entities.set_user_name(user_name)
            logging.info(f"User name set to {user_name}")

        # Main conversation loop
        while True:
            # Get user input
            user_name = self.entities.get_user_name()
            if user_name:
                question = input(f"{user_name}: ")
            else:
                question = input("You: ")

            # Log the user input (question)
            logging.info(f"User Input: {question}")

            # Check for special commands
            if question.lower() == "clear memory":
                logging.info("Clearing memory...")
                self.memory.vectorstore.delete_collection()  # Clear Chroma collection
                print("Conversation memory cleared.")
                logging.info("Memory cleared.")

            if question.lower() in ["quit", "exit", "bye"]:
                logging.info("Exiting conversation loop...")
                break

            # Load relevant context from memory using embeddings
            logging.info("Loading context from memory...")
            context = self.memory.load_memory_variables({"question": question})
            chat_history = context.get("history", "")
            logging.info(f"Loaded chat history: {chat_history}")

            # Determine if a search is needed, a skill should be used,
            # or the knowledge base should be queried
            logging.info("Routing question...")
            response = self.router.route(question, chat_history, self.available_skills)
            logging.info("Routing complete.")

            # Check if the response is None (meaning no route was found)
            if response is None:
                logging.info("No route found, defaulting response...")
                response = "I'm not sure how to answer that."
                logging.info(f"Default response: {response}")

            # Format the main prompt
            logging.info("Formatting prompt...")
            formatted_prompt = MAIN_PROMPT.format(
                chat_history=chat_history,
                question=question,
                search_results=response,
                user_name=user_name
            )
            logging.info(f"Formatted prompt: {formatted_prompt}")

            # Generate the agent's response using streaming output
            print("Agent: ", end="")
            response = ""
            logging.info("Generating response...")
            for chunk in self.llm.stream(formatted_prompt, temperature=0.7):
                response += chunk
                print(chunk, end="", flush=True)
                logging.info(f"Response chunk: {chunk}")
            logging.info("Response generation complete.")
            print()

            # Log the response
            logging.info(f"Response: {response}")

            # Update the chat history
            chat_history += f"{user_name}: {question}\nAgent: {response}\n"

            # Update the context vectorstore 
            logging.info("Updating context...")
            update_context(question, chat_history) 
            logging.info("Context updated.")

            # Save the current interaction to memory (using embeddings)
            logging.info("Saving context to memory...")
            self.memory.save_context({"input": question}, {"output": response})
            logging.info("Context saved.")
